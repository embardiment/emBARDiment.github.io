<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="EmBARDiment: an Embodied AI Agent for Productivity in XR.">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EmBARDiment: an Embodied AI Agent for Productivity in XR.</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            EmBARDiment: an Embodied AI Agent <br>for Productivity in XR
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://riccardobovo.com/" target="_blank">Riccardo Bovo</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://riccardobovo.com/" target="_blank">Steven Abreu</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://karan-ahuja.com/" target="_blank">Karan Ahuja</a><sup>1,4</sup>,
            </span>     
            <span class="author-block">
              <a href="https://www.ejgonzalez.me/" target="_blank">Eric J. Gonzalez</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/litecheng/" target="_blank">Li-Te Cheng</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://margonzalezfranco.github.io/" target="_blank">Mar Gonzalez-Franco</a><sup>1</sup>
            </span>
          </div>
        
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>1</sup> Google, USA
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <sup>2</sup> Imperial College London, UK
            </span>
            <br />
            <span class="author-block">
              <sup>3</sup> University of Groningen, Netherlands
            </span>
            <br />
            <span class="author-block">
              <sup>4</sup> Northwestern University, USA
            </span>
            <br /><br />
            <span class="author-block">
              <a href="https://ieeevr.org/2025/" target="_blank">
                2025 IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
              </a>
            </span>
          </div>

        

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://embardiment.github.io/static/vr25a-sub1168-cam-embardiment.pdf" target="_blank" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link (Coming Soon). -->
              <span class="link-block">
                <a href="https://github.com/google/embardiment"  target="_blank" 
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            XR devices running chatbots powered by Large Language Models (LLMs) have the potential to become always-on agents that significantly enhance productivity scenarios. Current screen-based chatbots fail to fully utilize the comprehensive suite of natural inputs available in XR, including inward-facing sensor data. Instead, they over-rely on explicit voice or text prompts, sometimes paired with multi-modal data included in the query. We propose a solution that leverages an attention framework to implicitly derive context from user actions, eye gaze, and contextual memory within the XR environment. Our approach minimizes the need for explicitly engineered prompts, fostering intuitive and grounded interactions that provide deeper user insights for the chatbot.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!--/ Abstract. -->


<!-- Video. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Video</h2>
        <video id="teaser" autoplay loop playsinline controls style="max-width: 100%;">
          <source src="./static/videos/embardiment ieeevr.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div>
  </div>
</section>

<!-- Paper video.
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <h2 class="title is-3">Applications</h2>
            <img src="https://github.com/google/xr-objects/raw/main/docs/FigureXRObjects.jpg"
        class="interpolation-image" width="100%"/>
        </div>
            <p> <img  style="float: left;padding-right: 30px;" src="./static/images/applications.png"
              width="225px"/>Through AOI, we envision XR-Objects to be useful across a variety of real-world applications. 
              By enabling in situ digital interactions with non-instrumented analog objects, we can expand their utility 
              (e.g., enabling a pot to double as a cooking timer), better synthesize their relevant information 
              (e.g., comparing nutritional value), and overall enable richer interactivity and flexibility in everyday interactions. 
                Here we present five example application scenarios from a broad application space we envision that highlight the value 
                of XR-Objects.</p><br/>
          </div>
        </div>
      </div>  
    </div>
        
      <br/>
        <div id="results-carousel" class="carousel results-carousel" style="border-width:0px;">
         
          <div class="item item-steve" style="border-width:0px;">
            <div class="columns is-centered" style="border-width:0px;">
            <img poster="" id="steve"  src="./static/images/discover.png"
            width="70%"/>
          </div>
          </div>
          
          <div class="item item-shiba" style="border-width:0px;">
            <div class="columns is-centered" style="border-width:0px;">
              
            <img poster="" id="shiba" src="./static/images/productivity.png"
            width="80%">
          </div>
          </div>
          <div class="item item-chair-tp" style="border-width:0px;">
            <div class="item" style="border-width:0px;">
              <img poster="" id="chair-tp"  src="./static/images/learning.png"
              width="90%">
            </div>
          </div>
          <div class="item item-fullbody" style="border-width:0px;">
            <img poster="" id="fullbody" src="./static/images/IOT.png"
            width="95%">
          </div>      
        </div>
        <br/>
        <h2 class="subtitle has-text-centered">
          <span class="dnerf">XR-Objects</span> makes analog objects interactable as if they were digital.
        </h2>
      
  </div>
 
</div>
</section>
-->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Implementation</h2>
        <div class="content has-text-justified">
          
          <p><strong>EmBARDiment:</strong> A practical XR application that integrates speech-to-text, text-to-speech, gaze-driven saliency, and LLMs for enabling a voice-interactive AI agent within a multi-window XR environment. Developed in Unity and deployed on the Oculus Quest Pro, EmBARDiment incorporates an embodied AI agent to enhance user interaction through verbal and non-verbal cues, utilizing APIs like ChatGPT-4 and Google Cloud Text-to-Speech for immersive experiences.</p>
          
          <p><strong>Multimodal Interaction:</strong> The system supports seamless user interaction by leveraging multiple input and output modalities. Verbal requests are transcribed using Google Speech-to-Text API and processed by LLM APIs for context-aware responses. These are converted into speech and synchronized with the AI agent’s facial animations for an engaging experience. Visual feedback is displayed on UI panels for clarity.</p>
          
          <p><strong>Gaze-Driven Contextual Memory:</strong> EmBARDiment extends WindowMirror, capturing and processing window content with the Google Vision API. By correlating text with the user’s eye-tracking data, the system determines focus and maintains a saliency buffer for coherent responses. This dynamic contextual memory, cleared after each user request, enables personalized and relevant interactions grounded in real-time attention.</p>
          <img poster="" id="fullbody" src="./static/images/pipeline.svg"> 
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{bovo2025embardiment,
  title={Embardiment: an embodied ai agent for productivity in xr},
  author={Bovo, Riccardo and Abreu, Steven and Ahuja, Karan and Gonzalez, Eric J 
      and Cheng, Li-Te and Gonzalez-Franco, Mar},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
  pages={708--717},
  year={2025},
  organization={IEEE}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is built on top of the original<a
            href="https://github.com/nerfies/nerfies.github.io">  Nerfies, <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 </a> International License.
          </p>
          </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
